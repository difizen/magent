{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "889d98c9-bc87-4e10-b4fc-4deb8cc4d992",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46799807-e779-4482-a90b-36f2c587f8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取网页内容\n",
    "def load_webpage(url: str):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # 确保请求成功\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # 提取文本内容（可以根据需要调整）\n",
    "    paragraphs = soup.find_all('p')\n",
    "    text = \"\\n\".join([para.get_text() for para in paragraphs])\n",
    "    return text\n",
    "\n",
    "# 读取网页链接\n",
    "url = \"https://medium.com/@libro.development/libro-a-customizable-ai-notebook-ab31bd4197b9\"  # 替换为你想读取的网页链接\n",
    "webpage_content = load_webpage(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a9331f2-8116-4a13-ae48-4d9589aa3e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: faiss-cpu in /Users/johnny/anaconda3/envs/test/lib/python3.11/site-packages (1.9.0.post1)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /Users/johnny/anaconda3/envs/test/lib/python3.11/site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /Users/johnny/anaconda3/envs/test/lib/python3.11/site-packages (from faiss-cpu) (23.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b3bec39-2617-4f61-9f1d-d4039828fca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/np/009dffqj74vbz89c1xghgskh0000gp/T/ipykernel_61692/719554337.py:11: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings()\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "# 文本分割\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "chunks = text_splitter.split_text(webpage_content)\n",
    "\n",
    "# 创建文档对象列表\n",
    "documents = [Document(page_content=chunk) for chunk in chunks]\n",
    "\n",
    "# 创建向量存储\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vector_store = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b86ccc34-f793-490d-9659-5d6ad4b881b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/np/009dffqj74vbz89c1xghgskh0000gp/T/ipykernel_61692/2518069785.py:30: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 1.0. Use :meth:`~Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc.` instead.\n",
      "  agent = initialize_agent(\n"
     ]
    }
   ],
   "source": [
    "# 创建一个RAG工具\n",
    "@tool\n",
    "def rag_query(query: str) -> str:\n",
    "    \"\"\"\n",
    "    This function retrieves relevant information from the loaded webpage data\n",
    "    and generates a response using the language model.\n",
    "\n",
    "    :param query: The user's query\n",
    "    :return: The generated response based on the retrieved information\n",
    "    \"\"\"\n",
    "    # Assuming `vector_store` and `llm` are accessible in the scope\n",
    "    relevant_docs = vector_store.similarity_search(query)\n",
    "    context = \"\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "    response = llm(f\"{context}\\n\\nUser: {query}\\nAssistant:\")\n",
    "    return response\n",
    "\n",
    "# 设置语言模型\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "# 定义工具\n",
    "tools = [rag_query]\n",
    "\n",
    "# 初始化代理\n",
    "agent = initialize_agent(\n",
    "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "418b59b7-7ff2-429a-a2bb-cdcd007dcf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/johnny/Code/difizen/magent/packages/magent-ui-langchain/src\")\n",
    "\n",
    "from magent_ui_langchain import launch\n",
    "# launch(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f558ed80-a2fd-4f36-82d9-62f83e613d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Current log level is\n",
      "INFO:     Started server process [61692]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Server is running at http://localhost:9563/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h2>Server is running at: <a href='http://localhost:9563/'>http://localhost:9563/</a></h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUoAAAFKAQAAAABTUiuoAAACJ0lEQVR4nO2ZTYrjMBCFvxoLspQhB+ijyDfrM/UNrKP0ARqsZUDmzUJy+mdoSAJOAlO1CI79LR6YKj+9MnFh5T+XkuCoo4466qije6LWKwCsRrYAlO32tLsAR69BkyRpAfII9qqTAYMkSd/RfQQ4eg1aegvZ6/tBUA7tHW39tr8AR29DVyO/VGx6lABHf6vw80Z+qUGUUEW5hwBHb0CjpBmAErCJ1QCQVO8jwNHL0WxmZiOQlkHAIJtYmyW8hwBHL6k2Cb/ET3kcEGU1iN9TqYdrdbSft6bSz1ua46m5jO285Z7wedCvLiPNqyktyNL7QUasQbCa9hTg6A3frdZMB7UYI4+rNb9hI9i0vwBHL6otrRgEcfN/SZLmWNtV//twrY5ukzAu9ASjHCt5/AjAQUYZ9xXg6A29FSuao3owmJZBJFU0x9oQ760nQHtvpbeAQWijUPnlZFCOPSxM834CHL3NEwpWI70da/OEecQgDrI9BTh6W/LUv1uxbhYRIL0HeiT1NFr/d7S/j1iBEtDMas0O5vEuAhy9qNok3KLAoQpORreDx0qbjvN+Ahy9Hj3vjs1GaFvk1mqf4fyzaHX0vDuGEoB4MrKZfX3wNFodPVdaBrUGmxlEWu4twNHf65/dMbGiPAUsaTVRhuoO/lnQc/IkoICyDdWIH4E8fmCf2eHjtTp6TnWBHu325KlbDcBT3WdBf+6O1X+aHbyDAEcdddRRRx29Av0LuyT9iPLdjSAAAAAASUVORK5CYII=",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:9563 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:60031 - \"GET / HTTP/1.1\" 307 Temporary Redirect\n",
      "INFO:     127.0.0.1:60031 - \"GET /app/ HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:60032 - \"GET / HTTP/1.1\" 307 Temporary Redirect\n",
      "INFO:     127.0.0.1:60032 - \"GET /app/ HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:60204 - \"GET / HTTP/1.1\" 307 Temporary Redirect\n",
      "INFO:     127.0.0.1:60204 - \"GET /app/ HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:60521 - \"POST /api/v1/chat-stream HTTP/1.1\" 200 OK\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new None chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mTo answer the question about the characteristics of \"libro,\" I need to gather relevant information about it. I will perform a query to find out more about the features or characteristics associated with \"libro.\" \n",
      "\n",
      "Action: rag_query  \n",
      "Action Input: \"libro 特性\"  \u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/np/009dffqj74vbz89c1xghgskh0000gp/T/ipykernel_61692/2518069785.py:14: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = llm(f\"{context}\\n\\nUser: {query}\\nAssistant:\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mcontent='Libro 是一个可定制的笔记本解决方案，专为支持生成式 AI 功能而设计。它提供了商业级的用户体验，能够无缝集成 AI，使其易于融入开发环境，构建前沿的 AI 和数据科学解决方案。以下是 Libro 的一些主要特性：\\n\\n1. **AI 工作流**：\\n   - **Prompt Cells**：用户可以将 AI 集成到笔记本的执行逻辑中，实现无缝的 AI 工作流。\\n   - **解释器模式**：用户可以使用自然语言与笔记本互动，生成所需的结果。\\n\\n2. **多种产品形式**：\\n   - Libro 支持多种用例，既可以作为文档编辑器，也可以作为报告演示工具，随时轻松创建演示和报告。\\n\\n3. **轻松的数据集成**：\\n   - 用户可以使用 SQL 单元轻松执行数据操作。简单的配置即可连接到自己的数据库，支持在 SQL 中使用 Python 上下文变量，并将查询结果继续作为数据框使用，确保数据工作流的顺畅和高效。\\n\\n4. **功能齐全的代码编辑器**：\\n   - Libro 将继续利用其灵活性和易集成性，探索更多笔记本类产品的用例。将不断扩展对不同运行时的支持，并引入更好的交互体验控件，使 Libro 在用户体验方面成为最佳的笔记本产品。\\n\\n5. **开放框架**：\\n   - Libro 提供开放框架，支持自定义 UI 和执行内核，允许在所有模块层进行二次开发和定制。\\n\\n6. **丰富的附加功能**：\\n   - Libro 除了基本的笔记本功能外，还提供了丰富的附加功能，增强用户的工作体验。\\n\\n通过这些特性，Libro 旨在更好地满足数据和 AI 工作流的需求，推动基于笔记本的工作方式的未来发展。欢迎开发者参与构建 Libro 项目，更多信息请访问 [GitHub](https://github.com/difizen/libro) 或 [官方网站](https://libro.difizen.net/)。' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 468, 'prompt_tokens': 676, 'total_tokens': 1144, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None} id='run-2295ee3b-7e24-4e40-8c1d-414f0817031e-0' usage_metadata={'input_tokens': 676, 'output_tokens': 468, 'total_tokens': 1144, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer.  \n",
      "Final Answer: Libro 是一个可定制的笔记本解决方案，具有以下主要特性：  \n",
      "1. **AI 工作流**：支持将 AI 集成到笔记本的执行逻辑中，用户可以使用自然语言与笔记本互动。  \n",
      "2. **多种产品形式**：既可以作为文档编辑器，也可以作为报告演示工具。  \n",
      "3. **轻松的数据集成**：支持 SQL 单元执行数据操作，简化数据库连接。  \n",
      "4. **功能齐全的代码编辑器**：灵活性和易集成性，支持不同运行时。  \n",
      "5. **开放框架**：支持自定义 UI 和执行内核，允许二次开发。  \n",
      "6. **丰富的附加功能**：增强用户的工作体验。  \n",
      "这些特性旨在满足数据和 AI 工作流的需求，推动基于笔记本的工作方式的发展。\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "INFO:     127.0.0.1:60793 - \"GET / HTTP/1.1\" 307 Temporary Redirect\n",
      "INFO:     127.0.0.1:60793 - \"GET /app/ HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:60794 - \"POST /api/v1/chat-stream HTTP/1.1\" 200 OK\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new None chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mTo answer the question about the size of the model, I need to gather specific information regarding the model's parameters or architecture. \n",
      "Action: rag_query\n",
      "Action Input: \"模型有多大？\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mcontent='模型的大小可以根据具体的实现和使用的技术而有所不同。一般来说，生成式AI模型的大小可以从几百万参数到数十亿参数不等。例如，较小的模型可能只有几百万参数，而大型模型如GPT-3则有1750亿个参数。具体的模型大小取决于其设计目标和应用场景。如果您有特定的模型或框架在心中，请提供更多信息，以便我能给出更准确的回答。' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 105, 'prompt_tokens': 542, 'total_tokens': 647, 'prompt_tokens_details': {'cached_tokens': 0, 'audio_tokens': 0}, 'completion_tokens_details': {'reasoning_tokens': 0, 'audio_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None} id='run-c30c8e7a-77b1-4424-b8e4-3d80a79c0e62-0' usage_metadata={'input_tokens': 542, 'output_tokens': 105, 'total_tokens': 647, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer. \n",
      "Final Answer: 模型的大小可以根据具体的实现和使用的技术而有所不同。一般来说，生成式AI模型的大小可以从几百万参数到数十亿参数不等。例如，较小的模型可能只有几百万参数，而大型模型如GPT-3则有1750亿个参数。具体的模型大小取决于其设计目标和应用场景。\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "launch(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc353f0d-51ad-4a9e-a97d-b71b56a813e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
